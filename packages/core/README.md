# ğŸš€ NxScraper Core Engine

**Clean, plugin-based web scraping orchestrator**

---

## ğŸ—ï¸ Architecture

This is the **core orchestrator** using a plugin-based architecture. All scrapers are independent plugins that implement the `IScraper` interface.

### Why Plugin Architecture?

- âœ… **Loose coupling** - Core doesn't know about specific scrapers
- âœ… **Easy to extend** - Add scrapers without modifying core
- âœ… **Easy to test** - Mock scrapers independently  
- âœ… **Maintainable** - Small, focused components

---

## ğŸ“ Structure

```
core-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts              # Entry point - registers plugins
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â””â”€â”€ worker.ts         # Job processor (73 lines)
â”‚   â””â”€â”€ plugins/
â”‚       â””â”€â”€ plugin-manager.ts # Scraper registry (99 lines)
â””â”€â”€ package.json
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd core-engine
npm install
```

### 2. Configure Environment

```bash
cp . env.example .env
# Edit .env as needed
```

### 3. Build

```bash
npm run build
```

### 4. Run

```bash
# Development
npm run dev

# Production
npm start
```

---

## ğŸ”Œ Adding a New Scraper

### Step 1: Create Scraper Service

```bash
mkdir -p ../services/my-scraper/src
```

### Step 2: Implement IScraper

```typescript
// services/my-scraper/src/scraper.ts
import { IScraper, ScrapeOptions, ScrapeResult } from '../../../shared/types/scraper.interface';

export class MyScraper implements IScraper {
    name = 'my-scraper';
    version = '1.0.0';

    async canHandle(url: string): Promise<number> {
        // Return confidence score 0-1
        return url.includes('example.com') ? 0.9 : 0;
    }

    async scrape(options: ScrapeOptions): Promise<ScrapeResult> {
        // Your scraping logic
        return {
            success: true,
            data: { /* ... */ },
            metadata: {
                url: options.url,
                timestamp: new Date().toISOString(),
                executionTimeMs: 0,
                engine: this.name
            }
        };
    }

    async healthCheck(): Promise<boolean> {
        return true;
    }
}
```

### Step 3: Register in Core Engine

```typescript
// core-engine/src/index.ts
async function registerScrapers() {
    // ... existing scrapers
    
    // Add your scraper
    const { MyScraper } = await import('../../services/my-scraper/src/scraper');
    pluginManager.register(new MyScraper());
    logger.info('Registered: MyScraper');
}
```

**That's it!** No changes to worker or orchestration logic needed.

---

## ğŸ§ª Testing

```bash
# Run all tests
npm test

# Watch mode
npm run test:watch

# Coverage
npm run test:coverage
```

---

## ğŸ“Š How It Works

### Job Flow

```
1. Job arrives in BullMQ queue
        â†“
2. JobWorker.processJob(job)
        â†“
3. pluginManager.scrape(job.data)
        â†“
4. findBestScraper(url)
   â”œâ”€ Call canHandle() on each scraper
   â”œâ”€ Get confidence scores
   â””â”€ Return highest scoring scraper
        â†“
5. scraper.scrape(options)
        â†“
6. Return result
```

### Dynamic Scraper Selection

```typescript
// Each scraper implements canHandle()
UniversalScraper.canHandle("https://example.com") â†’ 0.5
GoogleScraper.canHandle("https://google.com/search") â†’ 1.0
TelegramScraper.canHandle("https://t.me/channel") â†’ 1.0

// Plugin manager selects highest score
pluginManager.findBest("https://google.com") â†’ GoogleScraper
```

---

## ğŸ“ Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `NODE_ENV` | Environment (development/production) | development |
| `REDIS_URL` | Redis connection string | redis://localhost:6379 |
| `WORKER_CONCURRENCY` | Number of concurrent jobs | 5 |
| `LOG_LEVEL` | Logging level (debug/info/warn/error) | info |

---

## ğŸ”§ Development

### Project Commands

```bash
npm run build          # Compile TypeScript
npm run dev            # Run with ts-node (development)
npm start              # Run compiled code (production)
npm test               # Run tests
npm run test:coverage  # Test with coverage report
```

---

## ğŸ“ˆ Architecture Benefits

### Before (Monolithic)
- âŒ 300-line worker file
- âŒ 9 direct imports
- âŒ if-else chains for scraper selection
- âŒ Hard to test
- âŒ Hard to extend

### After (Plugin-Based)
- âœ… 73-line worker file
- âœ… Zero direct scraper imports
- âœ… Dynamic selection via canHandle()
- âœ… Easy to test (mock interface)
- âœ… Easy to extend (add plugin, done!)

---

## ğŸ¤ Contributing

1. Create scraper service in `/services`
2. Implement `IScraper` interface
3. Register in `core-engine/src/index.ts`
4. Add tests
5. Submit PR

---

## ğŸ“š Documentation

- [Migration Plan](../docs/MIGRATION_PLAN.md)
- [Architecture Review](../docs/ARCHITECTURE_REVIEW.md)
- [Architecture Comparison](../docs/ARCHITECTURE_COMPARISON.md)

---

**Status:** âœ… Migration in progress  
**Version:** 2.0.0  
**License:** MIT
